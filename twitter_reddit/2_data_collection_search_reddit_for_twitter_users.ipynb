{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm, trange\n",
    "import requests\n",
    "import glob\n",
    "import urllib.parse\n",
    "import nltk\n",
    "from labelling_classifier import update_model\n",
    "from utills import edit_distance, extract_twitter_username\n",
    "from data_collection_methods import query_reddit_comments, query_reddit_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/twitter_reddit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_highlighted_text(text):\n",
    "    print(re.sub(r\"(my twitter account|twitter.com)\", '\\x1b[31m\\g<1>\\x1b[0m', text,  flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "after = str(int(datetime(2010, 1, 1).timestamp()))\n",
    "before = str(int(datetime.now().timestamp()))\n",
    "query = '\"my twitter account\"+\"twitter.com\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + 'to_collect.p', 'rb') as f:\n",
    "    (to_collect, skipped, already_scanned, before, after) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2218, 33, 2231, '1622733540', 1622579527)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_collect), len(skipped), len(already_scanned), before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # to_collect = []\n",
    "# skipped = []\n",
    "# already_scanned = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search in Reddit Comments\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_negatives = []\n",
    "new_positives = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    clf = update_model(new_positives, new_negatives)\n",
    "    new_negatives = []\n",
    "    new_positives = []\n",
    "    \n",
    "    res = query_reddit_comments(query, before, after)\n",
    "    for r in res:\n",
    "        if r['author'] in already_scanned:\n",
    "            continue\n",
    "        print('-' * 20)\n",
    "        twitter_un = extract_username(r['body'])\n",
    "        dist = edit_distance(twitter_un, r['author'])\n",
    "        if twitter_un == 'i':\n",
    "            print('Bad twitter username. Skipping...')\n",
    "            continue\n",
    "        p = clf.predict_proba([r])[0, 1]\n",
    "        print('Prediction:', p)\n",
    "        if dist < 0.01:\n",
    "            print('Reddit username:', r['author'])\n",
    "            \n",
    "            print('Twitter username:', twitter_un)\n",
    "            print('Auto added!')\n",
    "            to_collect.append((r['author'], twitter_un, r))\n",
    "            already_scanned.add(r['author'])\n",
    "            new_positives.append(r)\n",
    "            continue\n",
    "        if 'not my twitter' in r['body'].lower():\n",
    "            print('Auto skipped!')\n",
    "            new_negatives.append(r)\n",
    "            continue\n",
    "        print_highlighted_text(r['body'])\n",
    "        print('Reddit username:', r['author'])\n",
    "        twitter_un = extract_username(r['body'])\n",
    "        print('Twitter username:', twitter_un)\n",
    "        print('Edit Distance:', dist)\n",
    "        response = input('Save:1, Skip:0, Save-for-later:2, Blacklist Name:3')\n",
    "        if response == '1':\n",
    "            to_collect.append((r['author'], twitter_un, r))\n",
    "            already_scanned.add(r['author'])\n",
    "            new_positives.append(r)\n",
    "        elif response == '2':\n",
    "            skipped.append(r)\n",
    "        elif response == '0':\n",
    "            new_negatives.append(r)\n",
    "            continue\n",
    "        elif response == '3':\n",
    "            already_scanned.add(r['author'])\n",
    "    after = r['created_utc']            \n",
    "    with open(DATA_PATH + 'to_collect.p', 'wb') as f:\n",
    "        pickle.dump((to_collect, skipped, already_scanned, before, after), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the search with Reddit submissions\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "after = str(int(datetime(2010, 1, 1).timestamp()))\n",
    "before = str(int(datetime.now().timestamp()))\n",
    "query = '\"my twitter account\"+\"twitter.com\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while(True):\n",
    "    # For now lets just see what the label classifying model syas, it is not accurate enough to use it\n",
    "    clf = update_model(new_positives, new_negatives)\n",
    "    new_negatives = []\n",
    "    new_positives = []\n",
    "    \n",
    "    res = query_reddit_submissions(query, before, after)\n",
    "    for r in res:\n",
    "        # Workaround so that comments and submissions are interchangable in the rest of the code\n",
    "        r['body'] = r['selftext']\n",
    "        if r['author'] in already_scanned:\n",
    "            continue\n",
    "        print('-' * 20)\n",
    "        twitter_un = extract_username(r['body'])\n",
    "        dist = edit_distance(twitter_un, r['author'])\n",
    "        if twitter_un == 'i' or twitter_un is None:\n",
    "            print('Bad twitter username. Skipping...')\n",
    "            continue\n",
    "        p = clf.predict_proba([r])[0, 1]\n",
    "        \n",
    "        \n",
    "        if dist < 0.01:\n",
    "            print('Reddit username:', r['author'])\n",
    "            \n",
    "            print('Twitter username:', twitter_un)\n",
    "            print('Auto added!')\n",
    "            to_collect.append((r['author'], twitter_un, r))\n",
    "            already_scanned.add(r['author'])\n",
    "            new_positives.append(r)\n",
    "            continue\n",
    "        if 'not my twitter' in r['body'].lower():\n",
    "            print('Auto skipped!')\n",
    "            new_negatives.append(r)\n",
    "            continue\n",
    "        print('Title: ', r['title'])\n",
    "        print_highlighted_text(r['body'])\n",
    "        print('Reddit username:', r['author'])\n",
    "        twitter_un = extract_username(r['body'])\n",
    "        print('Twitter username:', twitter_un)\n",
    "        print('Edit Distance:', dist)\n",
    "        print('Prediction:', p)\n",
    "        response = input('Save:1, Skip:0, Save-for-later:2, Blacklist Name:3')\n",
    "        if response == '1':\n",
    "            to_collect.append((r['author'], twitter_un, r))\n",
    "            already_scanned.add(r['author'])\n",
    "            new_positives.append(r)\n",
    "        elif response == '2':\n",
    "            skipped.append(r)\n",
    "        elif response == '0':\n",
    "            new_negatives.append(r)\n",
    "            continue\n",
    "        elif response == '3':\n",
    "            already_scanned.add(r['author'])\n",
    "    after = r['created_utc']            \n",
    "    with open(DATA_PATH + 'to_collect.p', 'wb') as f:\n",
    "        pickle.dump((to_collect, skipped, already_scanned, before, after), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
