{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/twitter_reddit/'\n",
    "TWITTER_DATA_DIR = '../data/twitter_reddit/manual_dataset/twitter/'\n",
    "REDDIT_DATA_DIR = '../data/twitter_reddit/manual_dataset/reddit/'\n",
    "REDDIT_SUB_DATA_DIR = '../data/twitter_reddit/manual_dataset/reddit_submissions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(DATA_PATH + 'to_collect.p', 'rb') as f:\n",
    "    to_collect, _, _, _, _ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_users = [d[0] for d in to_collect]\n",
    "twitter_users = [d[1] for d in to_collect]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Twitter Data\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    alltweets = []\n",
    "    try:\n",
    "        new_tweets = api.user_timeline(screen_name=screen_name, tweet_mode=\"extended\", count=200)\n",
    "    except tweepy.RateLimitError:\n",
    "        print('Rate limitted. Wating...')\n",
    "        time.sleep(15 * 60)\n",
    "        \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    if len(alltweets) == 0:\n",
    "        return alltweets\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "#         print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        try:\n",
    "            new_tweets = api.user_timeline(screen_name = screen_name, count=200, max_id=oldest, tweet_mode=\"extended\")\n",
    "        except tweepy.RateLimitError:\n",
    "            print('Rate limitted. Wating...')\n",
    "            time.sleep(15 * 60)\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "#         print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "    return alltweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('XXXX', 'XXXX')\n",
    "auth.set_access_token('XXX', 'XXX')\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completed_users = [re.search('/twitter/(.*).jsonl', f).group(1) for f in glob.glob(TWITTER_DATA_DIR + '*.jsonl')]\n",
    "failed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for u in twitter_users:\n",
    "    try:\n",
    "        print(u)\n",
    "        if u in completed_users or u in failed:\n",
    "            continue\n",
    "        all_tweets = get_all_tweets(u)\n",
    "        tweets = [t for t in all_tweets if not hasattr(t, \"retweeted_status\")]\n",
    "        print(u, len(all_tweets), len(tweets))\n",
    "        completed_users.append(u)\n",
    "        if len(tweets) < 100:\n",
    "            continue\n",
    "        with open(TWITTER_DATA_DIR + u + '.jsonl', 'w') as f:\n",
    "            for t in tweets:\n",
    "                json.dump(t.full_text, f)\n",
    "                f.write('\\n')\n",
    "    except tweepy.RateLimitError:\n",
    "        time.sleep(15 * 60)\n",
    "    except Exception as e:\n",
    "        print('Failed: ', e)\n",
    "        failed.append(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Reddit Data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPushshiftSubmissions(author_name, after, before):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?author='+str(author_name)+'&size=1000&after='+str(after)+'&before='+str(before)\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    while str(r) == '<Response [429]>':\n",
    "        time.sleep(10)\n",
    "        print('waiting')\n",
    "        r = requests.get(url)\n",
    "        \n",
    "    data = json.loads(r.text)\n",
    "    \n",
    "    return data['data']\n",
    "\n",
    "\n",
    "def getPushshiftComMeta(author_name, after, before):\n",
    "    url = 'https://api.pushshift.io/reddit/search/comment/?author='+str(author_name)+'&size=1000&after='+str(after)+'&before='+str(before)\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    while str(r) == '<Response [429]>':\n",
    "        time.sleep(10)\n",
    "        print('waiting')\n",
    "        r = requests.get(url)\n",
    "        \n",
    "    data = json.loads(r.text)\n",
    "    \n",
    "    return data['data']\n",
    "\n",
    "def reddit_user_comments(author_name,after_date,before_date):\n",
    "    \n",
    "    data = getPushshiftComMeta(author_name, after_date, before_date)\n",
    "    \n",
    "    all_data_user = []\n",
    "\n",
    "    while len(data) > 0:\n",
    "        \"\"\"\n",
    "            Calls getPushshiftData() with the created date of the last submission\n",
    "        \"\"\"\n",
    "        for com in data:\n",
    "            all_data_user.append(com['body'])\n",
    "\n",
    "        ##Change the after\n",
    "        after = data[-1]['created_utc']\n",
    "        data = getPushshiftComMeta(author_name, after, before_date)\n",
    "        print(len(all_data_user))\n",
    "        if len(all_data_user) > 10000:\n",
    "            break\n",
    "        \n",
    "    return all_data_user\n",
    "\n",
    "\n",
    "def reddit_user_submissions(author_name,after_date,before_date):\n",
    "    \n",
    "    data = getPushshiftSubmissions(author_name, after_date, before_date)\n",
    "    \n",
    "    all_data_user = []\n",
    "\n",
    "    while len(data) > 0:\n",
    "        \"\"\"\n",
    "            Calls getPushshiftData() with the created date of the last submission\n",
    "        \"\"\"\n",
    "        for com in data:\n",
    "            text = com['title']\n",
    "            if 'selftext' in com:\n",
    "                text += '\\n' + com['selftext']\n",
    "            all_data_user.append(text)\n",
    "\n",
    "        ##Change the after\n",
    "        after = data[-1]['created_utc']\n",
    "        data = getPushshiftSubmissions(author_name, after, before_date)\n",
    "        print(len(all_data_user))\n",
    "        if len(all_data_user) > 10000:\n",
    "            break\n",
    "        \n",
    "    return all_data_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "after = str(int(datetime(2010, 1, 1).timestamp()))\n",
    "before = str(int(datetime.now().timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completed_users_reddit = [re.search('/reddit/(.*).jsonl', f).group(1) for f in glob.glob(REDDIT_DATA_DIR + '*.jsonl')]\n",
    "failed_reddit = ['[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failed_reddit.append('Trollkitten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for u in tqdm(reddit_users):\n",
    "    try:\n",
    "        print(u)\n",
    "        if u in completed_users_reddit or u in failed_reddit or 'Bot' in u:\n",
    "            continue\n",
    "        all_comments = reddit_user_comments(u, after, before)\n",
    "        print(u, len(all_comments))\n",
    "        completed_users_reddit.append(u)\n",
    "        if len(all_comments) < 100:\n",
    "            continue\n",
    "        with open(REDDIT_DATA_DIR + u + '.jsonl', 'w') as f:\n",
    "            for c in all_comments:\n",
    "                json.dump(c, f)\n",
    "                f.write('\\n')\n",
    "    except Exception as e:\n",
    "        print('Failed: ', e)\n",
    "        failed_reddit.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completed_users_reddit_submissions = [re.search('/reddit_submissions/(.*).jsonl', f).group(1) for f in glob.glob(REDDIT_SUB_DATA_DIR + '*.jsonl')]\n",
    "failed_reddit_submissions = ['[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for u in tqdm(reddit_users):\n",
    "    try:\n",
    "        print(u)\n",
    "        if u in completed_users_reddit_submissions or u in failed_reddit_submissions or 'bot' in u.lower():\n",
    "            continue\n",
    "        all_subs = reddit_user_submissions(u, after, before)\n",
    "        print(u, len(all_subs))\n",
    "        completed_users_reddit_submissions.append(u)\n",
    "        if len(all_subs) < 50:\n",
    "            continue\n",
    "        with open(REDDIT_SUB_DATA_DIR + u + '.jsonl', 'w') as f:\n",
    "            for c in all_subs:\n",
    "                json.dump(c, f)\n",
    "                f.write('\\n')\n",
    "    except Exception as e:\n",
    "        print('Failed: ', e)\n",
    "        failed_reddit_submissions.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [u for u in twitter_users if u in saved_twitter_users]\n",
    "[u for u in reddit_users if u in saved_reddit_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "saved_twitter_users = [re.search('/twitter/(.*).jsonl', f).group(1) for f in glob.glob(TWITTER_DATA_DIR + '*.jsonl')]\n",
    "saved_reddit_users = [re.search('/reddit/(.*).jsonl', f).group(1) for f in glob.glob(REDDIT_DATA_DIR + '*.jsonl')]\n",
    "\n",
    "reddit_to_twitter = {\n",
    "    reddit_users[i]:twitter_users[i] \n",
    "    for i in range(len(reddit_users)) \n",
    "    if reddit_users[i] in saved_reddit_users and twitter_users[i] in saved_twitter_users}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit_to_twitter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
