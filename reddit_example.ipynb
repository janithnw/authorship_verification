{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from features import prepare_entry, merge_entries\n",
    "from utills import chunker, cartesian_product\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "TEMP_DIR = 'temp_data/'\n",
    "\n",
    "MULTIDOC_MODEL_PATH = TEMP_DIR + 'reddit/multidoc_10/model_10.p'\n",
    "SINGLEDOC_MODEL_PATH = TEMP_DIR + 'reddit/unchunked/model.p'\n",
    "chunk_sz = 10 # 10 Reddit comments per chunk, a comment is on avg 33 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(SINGLEDOC_MODEL_PATH, 'rb') as f:\n",
    "    (clf, transformer, scaler, secondary_scaler, _) = pickle.load(f)\n",
    "    \n",
    "with open(MULTIDOC_MODEL_PATH, 'rb') as f:\n",
    "    (clf_multi, transformer_multi, scaler_multi, secondary_scaler_multi, _) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_sets_A = [\n",
    "    ['This is some random text!!!'] * 20,\n",
    "    ['Another set of random comments! :) :)'] * 25,\n",
    "    ['More Reddit comments.'] * 27,\n",
    "    ['Moderation is also conducted by community-specific moderators, who are not Reddit employees'] * 30\n",
    "]\n",
    "usernames_A = ['user_A', 'user_B', 'user_C', 'user_D']\n",
    "doc_sets_B = [\n",
    "    ['Reddit is a network of communities where people can dive into their interests.'] * 20,\n",
    "    ['Posts are organized by subject into user-created boards called \"communities\" or \"subreddits\".'] * 23,\n",
    "    ['More Reddit comments.'] * 30,\n",
    "]\n",
    "usernames_B = ['user_L', 'user_M', 'user_N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunk_and_preprocess(texts, texts_per_chunk):\n",
    "    \"\"\"\n",
    "        Takes a list of strings (Reddit comments), groups them into chunks of\n",
    "        size `texts_per_chunk`, and preprocesses them\n",
    "    \"\"\"\n",
    "    return [\n",
    "        prepare_entry('\\n'.join(d), mode='accurate', tokenizer='casual') \n",
    "        for d in chunker(texts, texts_per_chunk)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24994e7c7e404fbcb22740fbff06bb00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e35d536ed1a407d9b78edb62e274458"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_A = [chunk_and_preprocess(docs, chunk_sz) for docs in tqdm(doc_sets_A)]\n",
    "preprocessed_B = [chunk_and_preprocess(docs, chunk_sz) for docs in tqdm(doc_sets_B)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(usernames, preprocessed, vector_path, vector_path_multi,\n",
    "             transformer, scaler, transformer_multi, scaler_multi):\n",
    "    total_chunks = 0\n",
    "    user_bounds = {}\n",
    "    user_to_idx = {}\n",
    "    for i, user in enumerate(usernames):\n",
    "        chunks = preprocessed[i]\n",
    "        user_bounds[user] = (total_chunks, total_chunks + len(chunks))\n",
    "        user_to_idx[user] = i\n",
    "        total_chunks += len(chunks)\n",
    "    \n",
    "    x_shape_multi = (total_chunks, len(transformer_multi.get_feature_names()))\n",
    "    x_shape = (len(usernames), len(transformer.get_feature_names()))\n",
    "    \n",
    "    XX_multi = np.memmap(vector_path_multi, dtype='float32', mode='w+', shape=x_shape_multi)\n",
    "    XX = np.memmap(vector_path, dtype='float32', mode='w+', shape=x_shape)\n",
    "    \n",
    "\n",
    "    for i, user in enumerate(tqdm(usernames)):\n",
    "        chunks = preprocessed[i]\n",
    "        s, e = user_bounds[user]\n",
    "        XX_multi[np.arange(s, e), :] = scaler_multi.transform(transformer_multi.transform(chunks).todense())\n",
    "        \n",
    "        XX[i, :] = scaler.transform(transformer.transform([merge_entries(chunks)]).todense())[0, :]\n",
    "        i += 1\n",
    "    return XX, XX_multi, user_to_idx, user_bounds, x_shape, x_shape_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d550355a6e2740df904f72b845a881a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "XX_A, XX_multi_A, user_to_idx_A, user_bounds_A, x_shape_A, x_shape_multi_A = vectorize(\n",
    "    usernames_A,\n",
    "    preprocessed_A,\n",
    "    'temp_data/reddit_example/XX_A.npy',\n",
    "    'temp_data/reddit_example/XX_A_multi.npy',\n",
    "    transformer,\n",
    "    scaler,\n",
    "    transformer_multi,\n",
    "    scaler_multi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87ba79e52fa45c2ab3e638c90edfa8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "XX_B, XX_multi_B, user_to_idx_B, user_bounds_B, x_shape_B, x_shape_multi_B = vectorize(\n",
    "    usernames_B,\n",
    "    preprocessed_B,\n",
    "    'temp_data/reddit_example/XX_B.npy',\n",
    "    'temp_data/reddit_example/XX_B_multi.npy',\n",
    "    transformer,\n",
    "    scaler,\n",
    "    transformer_multi,\n",
    "    scaler_multi\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of users we are going to compare\n",
    "jobs = []\n",
    "for a in usernames_A:\n",
    "    for b in usernames_B:\n",
    "        jobs.append((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70970e743e7840b4b497567280232ac8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "probs_single_doc = []\n",
    "\n",
    "    \n",
    "inter_probs_mean = []\n",
    "inter_probs_std = []\n",
    "\n",
    "intraA_probs_mean = []\n",
    "intraA_probs_std = []\n",
    "\n",
    "intraB_probs_mean = []\n",
    "intraB_probs_std = []\n",
    "pred_lengths = []\n",
    "    \n",
    "    \n",
    "for user_a, user_b in tqdm(jobs):\n",
    "\n",
    "    start_a, end_a = user_bounds_A[user_a]\n",
    "    start_b, end_b = user_bounds_B[user_b]\n",
    "    \n",
    "    #Inter A - B\n",
    "    l = []\n",
    "    idxs = cartesian_product(range(start_a, end_a), range(start_b, end_b))\n",
    "    x_diff = secondary_scaler_multi.transform(np.abs(XX_multi_A[idxs[:, 0]] - XX_multi_B[idxs[:, 1]]))\n",
    "    x_diff[np.isnan(x_diff)]=0\n",
    "    p = clf_multi.predict_proba(x_diff)[:, 1]\n",
    "    inter_probs_mean.append(p.mean())\n",
    "    inter_probs_std.append(p.std())\n",
    "    l.append(len(p))\n",
    "    \n",
    "    # Intra A\n",
    "    idxs = cartesian_product(range(start_a, end_a), range(start_a, end_a))\n",
    "    idxs = np.array([(i, j) for i, j in idxs if i != j])\n",
    "    x_diff = secondary_scaler_multi.transform(np.abs(XX_multi_A[idxs[:, 0]] - XX_multi_A[idxs[:, 1]]))\n",
    "    x_diff[np.isnan(x_diff)]=0\n",
    "    p = clf_multi.predict_proba(x_diff)[:, 1]\n",
    "    intraA_probs_mean.append(p.mean())\n",
    "    intraA_probs_std.append(p.std())\n",
    "    l.append(len(p))\n",
    "    \n",
    "    # Intra B\n",
    "    idxs = cartesian_product(range(start_b, end_b), range(start_b, end_b))\n",
    "    idxs = np.array([(i, j) for i, j in idxs if i != j])\n",
    "    x_diff = secondary_scaler_multi.transform(np.abs(XX_multi_B[idxs[:, 0]] - XX_multi_B[idxs[:, 1]]))\n",
    "    x_diff[np.isnan(x_diff)]=0\n",
    "    p = clf_multi.predict_proba(x_diff)[:, 1]\n",
    "    intraB_probs_mean.append(p.mean())\n",
    "    intraB_probs_std.append(p.std())\n",
    "    l.append(len(p))\n",
    "    \n",
    "    pred_lengths.append(l)\n",
    "    \n",
    "    p = clf.predict_proba(secondary_scaler.transform(np.abs(XX_A[[user_to_idx_A[user_a]], :] - XX_B[[user_to_idx_B[user_b]], :])))[0, 1]\n",
    "    probs_single_doc.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inter_probs_mean = np.array(inter_probs_mean)\n",
    "intraA_probs_mean = np.array(intraA_probs_mean)\n",
    "intraB_probs_mean = np.array(intraB_probs_mean)\n",
    "inter_probs_std = np.array(inter_probs_std)\n",
    "intraA_probs_std = np.array(intraA_probs_std)\n",
    "intraB_probs_std = np.array(intraB_probs_std)\n",
    "pred_lengths = np.array(pred_lengths)\n",
    "\n",
    "\n",
    "probs_single_doc = np.array(probs_single_doc)\n",
    "\n",
    "n_a = pred_lengths[:, 0]\n",
    "n_b = pred_lengths[:, 1]\n",
    "n_ab = pred_lengths[:, 2]\n",
    "\n",
    "intra_probs_mean = (intraA_probs_mean * n_a + intraB_probs_mean * n_b)/ (n_a + n_b)\n",
    "intra_probs_std = (\n",
    "        n_a * (intraA_probs_std ** 2 + (intraA_probs_mean - intra_probs_mean)**2) + \n",
    "        n_b * (intraB_probs_std ** 2 + (intraB_probs_mean - intra_probs_mean)**2)\n",
    "    ) / (n_a + n_b)\n",
    "\n",
    "\n",
    "pooled_mean = (intra_probs_mean * (n_a + n_b) + inter_probs_mean * n_ab)/ (n_a + n_b + n_ab)\n",
    "pooled_std = (\n",
    "        (n_a + n_b) * (intra_probs_mean ** 2 + (intra_probs_mean - pooled_mean)**2) + \n",
    "        n_ab * (inter_probs_mean ** 2 + (inter_probs_mean - pooled_mean)**2)\n",
    "    ) / (n_a + n_b + n_ab)\n",
    "\n",
    "intra_inter_diff = (1 - np.abs(inter_probs_mean - intra_probs_mean))\n",
    "aggr_score = probs_single_doc * intra_inter_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_C -  user_N :  0.9999887458735573\n",
      "user_C -  user_L :  0.9997480774211217\n",
      "user_A -  user_L :  0.9016050442139604\n",
      "user_A -  user_N :  0.9634288008548452\n",
      "user_B -  user_L :  0.8528202406385471\n",
      "user_B -  user_N :  0.9215806134544478\n",
      "user_B -  user_M :  0.9840362957294717\n",
      "user_A -  user_M :  0.967831465539838\n",
      "user_D -  user_N :  0.9668948688100941\n",
      "user_C -  user_M :  0.9949778048873532\n",
      "user_D -  user_L :  0.9454004039782098\n",
      "user_D -  user_M :  0.9614113149343242\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(-aggr_score):\n",
    "    user_a, user_b = jobs[i]\n",
    "    print(user_a, '- ', user_b, ': ', intra_inter_diff[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
