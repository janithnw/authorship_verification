{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import re\n",
    "import os.path\n",
    "import numpy as np\n",
    "from features import prepare_entry\n",
    "from features import merge_entries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from features import merge_entries, get_transformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/wikipedia/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wiki_preprocess(text):\n",
    "    text = re.sub(r'\\[\\[User:.*\\]\\]\\s\\(\\[\\[User.*\\]\\].*\\(UTC\\)', '', text)\n",
    "    text = re.sub(r'\\[\\[.*\\]\\]', 'PAGE', text)\n",
    "    text = re.sub(r'\\d{2}:\\d{2},\\s\\d+\\s\\w+\\s\\d{4}\\s\\(UTC\\)', 'DATE', text)\n",
    "    text = re.sub(r'==.*==(\\n)*', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_pairs = []\n",
    "username_pairs = []\n",
    "labels = []\n",
    "for directory in glob.glob(DATA_PATH + 'Sockpuppet/*'):\n",
    "    fps = glob.glob(os.path.join(directory, '*'))\n",
    "    usernames = [os.path.basename(os.path.normpath(fp)) for fp in fps]\n",
    "    files_1 = [f for f in glob.glob(os.path.join(fps[0], '*.*')) if 'Categories.txt' not in f and 'Articles.txt' not in f]\n",
    "    files_2 = [f for f in glob.glob(os.path.join(fps[1], '*.*')) if 'Categories.txt' not in f and 'Articles.txt' not in f]\n",
    "    docs_1 = [wiki_preprocess(open(f).read()) for f in files_1]\n",
    "    docs_2 = [wiki_preprocess(open(f).read()) for f in files_2]\n",
    "    if len('\\n'.join(docs_1)) < 10 or  len('\\n'.join(docs_2)) < 10:\n",
    "        continue\n",
    "    doc_pairs.append((docs_1, docs_2))\n",
    "    username_pairs.append(usernames)\n",
    "    labels.append(1)\n",
    "    \n",
    "for directory in glob.glob(DATA_PATH + 'Non-sockpuppet/*'):\n",
    "    fps = glob.glob(os.path.join(directory, '*'))\n",
    "    usernames = [os.path.basename(os.path.normpath(fp)) for fp in fps]\n",
    "    files_1 = [f for f in glob.glob(os.path.join(fps[0], '*.*')) if 'Categories.txt' not in f and 'Articles.txt' not in f]\n",
    "    files_2 = [f for f in glob.glob(os.path.join(fps[1], '*.*')) if 'Categories.txt' not in f and 'Articles.txt' not in f]\n",
    "    docs_1 = [wiki_preprocess(open(f).read()) for f in files_1]\n",
    "    docs_2 = [wiki_preprocess(open(f).read()) for f in files_2]\n",
    "    if len('\\n'.join(docs_1)) < 10 or  len('\\n'.join(docs_2)) < 10:\n",
    "        continue\n",
    "    doc_pairs.append((docs_1, docs_2))\n",
    "    username_pairs.append(usernames)\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd81872bd0264137bd0fe8cc415e5b9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e927acfba0ed4d288c41981f2143f72c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs_1_preprocessed = [prepare_entry('\\n'.join(d1)) for d1, d2 in tqdm(doc_pairs)]\n",
    "docs_2_preprocessed = [prepare_entry('\\n'.join(d2)) for d1, d2 in tqdm(doc_pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_1_preprocessed = np.array(docs_1_preprocessed)\n",
    "docs_2_preprocessed = np.array(docs_2_preprocessed)\n",
    "username_pairs = np.array(username_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(labels))\n",
    "labels = labels[p]\n",
    "docs_1_preprocessed = docs_1_preprocessed[p]\n",
    "docs_2_preprocessed = docs_2_preprocessed[p]\n",
    "username_pairs = username_pairs[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1857be13fda4b1296c1db5fb4515790"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7583333333333333\n",
      "AUC: 0.79296875\n",
      "AUC: 0.7411764705882353\n",
      "AUC: 0.8458333333333332\n",
      "AUC: 0.7686274509803921\n",
      "AUC: 0.9138655462184874\n",
      "AUC: 0.6666666666666666\n",
      "AUC: 0.8179824561403509\n",
      "AUC: 0.8376068376068376\n",
      "AUC: 0.8844537815126051\n",
      "AUC: 0.7815126050420168\n",
      "AUC: 0.7166666666666668\n",
      "AUC: 0.8636363636363636\n",
      "AUC: 0.8290598290598291\n",
      "AUC: 0.7456140350877193\n",
      "AUC: 0.9411764705882353\n",
      "AUC: 0.7815126050420169\n",
      "AUC: 0.8289473684210527\n",
      "AUC: 0.8109243697478992\n",
      "AUC: 0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "kf = KFold(n_splits=20)\n",
    "for train_index, test_index in tqdm(kf.split(labels), total=kf.get_n_splits()):\n",
    "    transformer = get_transformer()\n",
    "    scaler = StandardScaler()\n",
    "    X = transformer.fit_transform(np.concatenate([docs_1_preprocessed[train_index], docs_2_preprocessed[train_index]]))\n",
    "    X = scaler.fit_transform(X.todense())\n",
    "    X1 = X[:len(train_index)]\n",
    "    X2 = X[len(train_index):]\n",
    "\n",
    "    secondary_scaler = StandardScaler()\n",
    "    X_diff = np.abs(X1 - X2)\n",
    "    X_diff = secondary_scaler.fit_transform(X_diff)\n",
    "    \n",
    "    clf = SGDClassifier(loss='log', alpha=0.01)\n",
    "    clf.fit(X_diff, labels[train_index])\n",
    "    \n",
    "    X1 = scaler.transform(transformer.transform(docs_1_preprocessed[test_index]).todense())\n",
    "    X2 = scaler.transform(transformer.transform(docs_2_preprocessed[test_index]).todense())\n",
    "    X_diff_test = secondary_scaler.transform(np.abs(X1 - X2))\n",
    "    probs = clf.predict_proba(X_diff_test)[:, 1]\n",
    "    fpr, tpr, thresh = roc_curve(labels[test_index], probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print('AUC:', roc_auc)\n",
    "    aucs.append(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8481557760718966\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_diff, labels, test_size=0.33, random_state=42)\n",
    "clf = SGDClassifier(loss='log', alpha=0.01)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresh = roc_curve(y_test, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21126"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['char_distr__0p', 'char_distr__gr', \"pos_tag_distr__'' VBD ''\",\n",
       "       \"masked_stop_words_distr__'' VBD ''\", 'freq_func_words__across',\n",
       "       'masked_stop_words_distr__in such',\n",
       "       'masked_stop_words_distr__much -None-',\n",
       "       'masked_stop_words_distr__it might',\n",
       "       'masked_stop_words_distr__JJ page',\n",
       "       'masked_stop_words_distr__: for', 'pos_tag_distr__NN IN VB',\n",
       "       'masked_stop_words_distr__NNS of ``',\n",
       "       'masked_stop_words_distr__not really',\n",
       "       'masked_stop_words_distr__-None- date :',\n",
       "       'masked_stop_words_distr__across', 'char_distr__ :f',\n",
       "       'masked_stop_words_distr__. also',\n",
       "       'masked_stop_words_distr__for -None-',\n",
       "       'pos_tag_chunks_subtree_distr__VP[VBN]',\n",
       "       'masked_stop_words_distr__: is', 'char_distr__-20',\n",
       "       'pos_tag_distr__JJR CC JJR', 'char_distr__so',\n",
       "       'masked_stop_words_distr__NNS and the', 'char_distr__-ne',\n",
       "       'pos_tag_distr__TO PRP IN', 'char_distr__i> ',\n",
       "       'masked_stop_words_distr__: i would', 'freq_func_words__clearly',\n",
       "       'special_char_distr__<', 'masked_stop_words_distr__for some',\n",
       "       'masked_stop_words_distr__NN -None- the', 'char_distr__gua',\n",
       "       'char_distr__n).', 'masked_stop_words_distr__allow',\n",
       "       'char_distr__(mo', 'masked_stop_words_distr__would be a',\n",
       "       'masked_stop_words_distr__two JJ', 'special_char_distr__>',\n",
       "       'pos_tag_distr__VBZ TO PRP',\n",
       "       'masked_stop_words_distr__-None- for example', 'char_distr__300',\n",
       "       'masked_stop_words_distr__NNS is not', 'freq_func_words__unto',\n",
       "       'freq_func_words__allow', 'pos_tag_stats__tag_word_length_WRB',\n",
       "       'char_distr__on)', 'char_distr__p> ',\n",
       "       'masked_stop_words_distr__VBN the JJ', 'char_distr__67'],\n",
       "      dtype='<U50')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = np.array(transformer.get_feature_names())\n",
    "fnames[np.argsort(-np.abs(clf.coef_[0]))][:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
